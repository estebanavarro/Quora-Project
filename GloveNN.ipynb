{"cells":[{"metadata":{"trusted":true,"_uuid":"5dd0e1aa1164c7f67e5c93d2d11442c759fef586"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('max_colwidth', 240)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport re\nimport os\nimport operator\nimport sys\nprint(os.listdir(\"../input\"))\n\nimport matplotlib.pyplot as plt\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5db2f5942a45258a8297e571b13f990295056f91"},"cell_type":"code","source":"MAX_NB_WORDS = 120000\nGLOVE_DIR = \"../input/embeddings/glove.840B.300d\"\nEMBEDDING_DIM = 300\nMAX_LEN = 75","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98044466744d3f6dea85ee5d4dead3e41f516529"},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"trusted":true,"_uuid":"295ed1b221354bdc817e2fe7b5fe430c90c7301d"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ndf = pd.concat([train.drop('target',axis=1),test])\n# Comment after debugging, before committing:\n#df = df.sample(150000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce59cf855bf1d789f3f1db76ee53b6d6179fd32a"},"cell_type":"markdown","source":"## Clean Data"},{"metadata":{"trusted":true,"_uuid":"82340491c2063e0c6f545daa6a6e1b19c73e2010"},"cell_type":"code","source":"#X = traindf[\"question_text\"].fillna(\"_##_\").values\n#y = traindf[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60e2ed2169d8e877e6cac073211f67fbe7bd89fb"},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"trusted":true,"_uuid":"3000f62693163458155801e89124e11618746d92"},"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'))\nfor line in f:\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84e54206d41530c442257ee734206f45d71fd863"},"cell_type":"code","source":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cafad0cc41e8428d9ce614c0da80dda7a3d0bb45"},"cell_type":"code","source":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3012a0f25e2e25cfcd47ca861e63542628125d15"},"cell_type":"code","source":"vocab = build_vocab(df[\"question_text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a25a38b9e1d0382f25c663139ccc50012a6a844d"},"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ce619467f47da08545569285da599b3107617d"},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cefdc2079dfcbf9f5585ce79bdc239e8486714ff"},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \n                       \"aren't\": \"are not\",\n                       \"can't\": \"cannot\", \n                       \"'cause\": \"because\", \n                       \"could've\": \"could have\",\n                       \"couldn't\": \"could not\", \n                       \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \n                       \"don't\": \"do not\", \n                       \"hadn't\": \"had not\", \n                       \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \n                       \"he'd\": \"he would\",\n                       \"he'll\": \"he will\", \n                       \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \n                       \"how'll\": \"how will\", \n                       \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \n                       \"I'll\": \"I will\", \n                       \"I'm\": \"I am\", \n                       \"I've\": \"I have\", \n                       \"i'd\": \"i would\", \n                       \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \n                       \"i'm\": \"i am\", \n                       \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \n                       \"it'd\": \"it would\", \n                       \"it'll\": \"it will\", \n                       \"it's\": \"it is\", \n                       \"let's\": \"let us\", \n                       \"might've\": \"might have\",\n                       \"must've\": \"must have\", \n                       \"needn't\": \"need not\", \n                       \"o'clock\": \"of the clock\", \n                       \"shan't\": \"shall not\", \n                       \"she'd\": \"she would\",\n                       \"she'll\": \"she will\", \n                       \"she's\": \"she is\", \n                       \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \n                       \"that'd\": \"that would\", \n                       \"there's\": \"there is\", \n                       \"here's\": \"here is\",\n                       \"they'd\": \"they would\",\n                       \"they'll\": \"they will\", \n                       \"they're\": \"they are\", \n                       \"they've\": \"they have\", \n                       \"wasn't\": \"was not\", \n                       \"we'd\": \"we would\", \n                       \"we'll\": \"we will\", \n                       \"we're\": \"we are\", \n                       \"we've\": \"we have\", \n                       \"weren't\": \"were not\", \n                       \"what'll\": \"what will\", \n                       \"what're\": \"what are\",  \n                       \"what's\": \"what is\", \n                       \"what've\": \"what have\", \n                       \"when's\": \"when is\", \n                       \"when've\": \"when have\", \n                       \"where'd\": \"where did\", \n                       \"where's\": \"where is\", \n                       \"where've\": \"where have\", \n                       \"who'll\": \"who will\", \n                       \"who's\": \"who is\", \n                       \"who've\": \"who have\", \n                       \"why's\": \"why is\", \n                       \"won't\": \"will not\", \n                       \"would've\": \"would have\", \n                       \"wouldn't\": \"would not\", \n                       \"y'all\": \"you all\", \n                       \"you'd\": \"you would\", \n                       \"you'd've\": \"you would have\", \n                       \"you'll\": \"you will\", \n                       \"you'll've\": \"you will have\", \n                       \"you're\": \"you are\", \n                       \"you've\": \"you have\" }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fa77da354ad86c9866f73a926ec1d167d7f4096"},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b4314cb43910fc12ead75f5ad429dcc35fcd31a"},"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63276e71ba5e0d71031c4d8cb4c6ddd639b3660a"},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5a324ba537e084b9ad0123e13e3f757d25fb01e"},"cell_type":"code","source":"df['treated_question'] = df['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fae9fc1ce8321b3fd0cebb749a3646e84983da7","scrolled":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3380ea804e70f1b0cc508e85d94525ed14ef4c28"},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c92bdd4a4a52774e106dc6f27709f6f78bf76e2c"},"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6b6de7e3201e63d0b46ff98b7b00c1896ffbb64"},"cell_type":"code","source":"print(\"Glove :\")\nprint(unknown_punct(embeddings_index, punct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a8330b67dec2c212a0a402c34020b8ed02bbf47"},"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \n                 \"₹\": \"e\", \n                 \"´\": \"'\", \n                 \"°\": \"\", \n                 \"€\": \"e\", \n                 \"™\": \"tm\", \n                 \"√\": \" sqrt \", \n                 \"×\": \"x\", \n                 \"²\": \"2\", \n                 \"—\": \"-\", \n                 \"–\": \"-\", \n                 \"’\": \"'\",\n                 \"_\": \"-\", \n                 \"`\": \"'\", \n                 '“': '\"', \n                 '”': '\"', \n                 '“': '\"', \n                 \"£\": \"e\", \n                 '∞': 'infinity', \n                 'θ': 'theta', \n                 '÷': '/', \n                 'α': 'alpha', \n                 '•': '.', \n                 'à': 'a', \n                 '−': '-', \n                 'β': 'beta', \n                 '∅': '', \n                 '³': '3', \n                 'π': 'pi'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c80ee02600516c7287218ba16d6f92b6ba771f4e"},"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8774e5ba819876d8787728dbc1db91168872fe9"},"cell_type":"code","source":"df['treated_question'] = df['treated_question'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c20c8c73689f0d0eb56f739d126b689007ba9a3"},"cell_type":"code","source":"vocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90fcec7859921eb273cf0a2ef01bd2badec0399c"},"cell_type":"code","source":"oov_glove[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2623a4b437f76bd789be1f9937900ffe90e21c57"},"cell_type":"code","source":"mispell_dict = {'Quorans': 'Quora',\n                'cryptocurrencies': 'cryptocurrency',\n                'Redmi' :'cellphone',\n                'OnePlus' :'cellphone',\n                'Blockchain':'blockchain',\n                'Pokémon':'Pokemon',\n                'ethereum':'Ethereum',\n                'Qoura':'Quora',\n                'fiancé':'fiance',\n                'Litecoin': 'cryptocurrency',\n                'altcoin': 'cryptocurrency',\n                'Cryptocurrency': 'cryptocurrency',\n                'altcoins' : 'cryptocurrency',\n                'litecoin' : 'cryptocurrency',\n                'colour': 'color',\n                'centre': 'center',\n                'favourite': 'favorite',\n                'travelling': 'traveling',\n                'counselling': 'counseling',\n                'theatre': 'theater',\n                'cancelled': 'canceled',\n                'labour': 'labor',\n                'organisation': 'organization',\n                'wwii': 'world war 2',\n                'citicise': 'criticize',\n                'youtu ': 'youtube ',\n                'Qoura': 'Quora',\n                'sallary': 'salary',\n                'Whta': 'What',\n                'narcisist': 'narcissist',\n                'howdo': 'how do',\n                'whatare': 'what are',\n                'howcan': 'how can',\n                'howmuch': 'how much',\n                'howmany': 'how many',\n                'whydo': 'why do',\n                'doI': 'do I',\n                'theBest': 'the best',\n                'howdoes': 'how does',\n                'mastrubation': 'masturbation',\n                'mastrubate': 'masturbate',\n                \"mastrubating\": 'masturbating',\n                'pennis': 'penis',\n                'Etherium': 'Ethereum',\n                'narcissit': 'narcissist',\n                'bigdata': 'big data', \n                '2k17': '2017', \n                '2k18': '2018', \n                'qouta': 'quota', \n                'exboyfriend': 'ex boyfriend', \n                'airhostess': 'air hostess', \n                \"whst\": 'what', 'watsapp': 'whatsapp', \n                'demonitisation': 'demonetization', \n                'demonitization': 'demonetization', \n                'demonetisation': 'demonetization'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37ea77815c24c0121e1c06363a244dcbf19ed7bb"},"cell_type":"code","source":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdec2a7eccee39ee6c0bfa394e954dcd2b116c3c"},"cell_type":"code","source":"df['treated_question'] = df['treated_question'].apply(lambda x: correct_spelling(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a2fdf9c38814a4bb24782bbd28f7ef00847aba4"},"cell_type":"code","source":"vocab = build_vocab(df['treated_question'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dfe98bd919cf19194363eb1187ba753acde3093"},"cell_type":"code","source":"y = pd.read_csv('../input/train.csv')['target']\nX = df[:len(y)]['treated_question']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa92147fb0ed68e570b028fe898c8d19cae2c159"},"cell_type":"code","source":"%%time\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n# tokenizer: words --> word indices\n\ntexts, labels = X, y\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(list(X))\nsequences = tokenizer.texts_to_sequences(X)\n\nsequences = pad_sequences(sequences, maxlen=MAX_LEN)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74c6f5a2027aa17b71ca19d82c20d22039125a33"},"cell_type":"code","source":"from keras import backend as K\nK.tensorflow_backend._get_available_gpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e57ddf2cf4dae3ebacc22ed830a7c5bc1de9229"},"cell_type":"code","source":"# Example sequence\nsequences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4eba9aa8e5c089a0c2a583caee1d6cc9b4c760e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(sequences, labels, test_size=0.2)\ny_train_bin = to_categorical(y_train)\ny_val_bin = to_categorical(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6b355143597a109cf863ebfe8e93c106e665c8b"},"cell_type":"code","source":"print('Vector for \\'apple\\' =')\nprint(str(embeddings_index['apple'][0:10]) + '... plus 290 more features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85c8c73d5b423bd2e776852fb3ff50987eb5d614"},"cell_type":"code","source":"%%time\n# embedding_matrix: word index --> word vector\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f3587d3920f6078957f65cd8a6a6baff0b6f81f"},"cell_type":"code","source":"ai = word_index['apple']\nprint('\\'apple\\' is word number: ', ai)\nprint('The vector for word number ', ai, ' is:')\nprint(str(embedding_matrix[ai][0:10]) + '... plus 290 more features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b495e4c830e8bb23116f46c2a72ab713a17bfa47"},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_LEN,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"988a446be94570a32dac167a12f6175bdbee3047"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Input, Bidirectional, CuDNNGRU, GlobalMaxPool1D, MaxPooling1D, Flatten, Dense, Dropout\nfrom keras.models import Model\nfrom keras import regularizers\n\nmodel = Sequential()\n#model.add(Input(shape=(MAX_LEN,), dtype='int32'))\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(CuDNNGRU(64, return_sequences=True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(32, activation=\"sigmoid\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32, activation=\"sigmoid\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2, activation=\"softmax\"))\n\n#model = Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8d7a08555cce44195005043f2f50753a142787e"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"279bffb596f2ce9386f4ee620284a6dd62608433"},"cell_type":"code","source":"from keras import callbacks\nfrom sklearn import metrics\nclass cbmetrics(callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        scores = self.model.predict(X_val, batch_size=1024, verbose=1)[:,1]\n        pr, re, th = metrics.precision_recall_curve(y_val, scores)\n        pr, re, th = pr[:-2], re[:-2], th[:-1]\n        fs = 2*np.divide(np.multiply(pr, re), np.add(pr, re))\n        i = np.argmax(fs)\n        self.val_f1s.append(np.max(fs))\n        self.val_recalls.append(re[i])\n        self.val_precisions.append(pr[i])\n        print(\"f score: \", np.max(fs))\n        \nf1 = cbmetrics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22f937385bec1104638852ccfd49e3b7732d7bba","scrolled":false},"cell_type":"code","source":"history = model.fit(X_train, y_train_bin, validation_data=(X_val, y_val_bin),\n          epochs=3, batch_size=256, verbose=1, callbacks=[f1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee35c4aeda512c123c1fe476f6d682d82755f336"},"cell_type":"code","source":"# Get training and test loss histories\ntraining_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, val_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faf96690f4ea8ddb68088df0af68556420454ed6"},"cell_type":"code","source":"scores = model.predict(X_val, batch_size=1024, verbose=1)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22611eac29a322e804d67a5b65c9c2eb5ab3bb25"},"cell_type":"code","source":"from sklearn import metrics\npr, re, th = metrics.precision_recall_curve(y_val, scores)\npr, re, th = pr[:-2], re[:-2], th[:-1]\nfs = 2*np.divide(np.multiply(pr, re), np.add(pr, re))\n\nplt.figure(figsize=(12,8))\nplt.plot(th, pr, label = 'precision')\nplt.plot(th, re, label = 'recall')\nplt.plot(th, fs, label = 'f score')\nplt.xlabel('Threshold')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfd35a88a5124836b0b219bbb1f43402741d4bd0","scrolled":true},"cell_type":"code","source":"opt_thr = th[np.argmax(fs)]\nprint(opt_thr, np.max(fs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e1bfdfaeb54fa5d56f358e37293476391ddfb5"},"cell_type":"code","source":"# Next 2 cells are from https://www.kaggle.com/frankkloster/time-to-put-your-gloves-on\ndf_test = df[len(y):]\nx_test = df_test[\"treated_question\"]\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec6014eca8454a3c842792e7a86932f3a8574359"},"cell_type":"code","source":"y_test = model.predict([x_test], batch_size=1024, verbose=1)[:,1]\ny_test = (y_test > opt_thr).astype(int)\ndf_test = pd.DataFrame({\"qid\": df_test[\"qid\"].values})\ndf_test['prediction'] = y_test\ndf_test.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a80aced004fa1e098d3535d85605e828ce9b82c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01357b96a7cf7dcb5b2663affa0744c535755039"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}